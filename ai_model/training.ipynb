{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b58a39-4998-4e7c-8753-1e17c65ba347",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "In this section, we import all necessary Python libraries required for our project. These libraries facilitate neural network construction (PyTorch), data handling (NumPy), and operating system interactions (os). We also import utilities for dataset management and splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee8df0-7e94-4e09-bfd4-db4659af84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6feda30-87da-4710-922e-e481ddfa8400",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "This cell defines the `load_data` function to navigate through dataset directories, load binary sensor data files, and preprocess them. The preprocessing involves reshaping and scaling the data:\n",
    "- Accelerometer data is scaled by 2048 to convert to 'g' units.\n",
    "- Gyroscopic data is scaled by 16.4 to convert to degrees per second.\n",
    "We store the scaled data and associated labels in NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94579728-8bcd-4566-8382-8d509f63a6b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "    data = []\n",
    "    labels = []\n",
    "    categories = ['axel', 'flip', 'loop', 'lutz', 'salchow', 'toe']\n",
    "    for idx, category in enumerate(categories):\n",
    "        path = os.path.join(directory, category)\n",
    "        for file in os.listdir(path):\n",
    "            file_path = os.path.join(path, file)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                raw_data = np.fromfile(f, dtype=np.int16).reshape(150, 30)\n",
    "                raw_data = raw_data.astype(np.float64)  # Convert to float64 before scaling\n",
    "                # Apply scaling for accelerometer and gyroscopic data\n",
    "                for i in range(5):\n",
    "                    # Accelerometer indices: 0, 1, 2, 6, 7, 8, ..., 24, 25, 26\n",
    "                    # Gyroscopic indices: 3, 4, 5, 9, 10, 11, ..., 27, 28, 29\n",
    "                    acc_indices = [i*6, i*6+1, i*6+2]\n",
    "                    gyro_indices = [i*6+3, i*6+4, i*6+5]\n",
    "                    raw_data[:, acc_indices] /= 2048.0  # Scale accelerometer data\n",
    "                    raw_data[:, gyro_indices] /= 16.4   # Scale gyroscopic data\n",
    "                data.append(raw_data)\n",
    "                labels.append(idx)\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "data, labels = load_data('../data/labeled_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316f4aa-df4b-469c-a267-4faa504d4f58",
   "metadata": {},
   "source": [
    "## Define the LSTM Model\n",
    "Below, we define our `JumpClassifier` class, which is a PyTorch neural network model consisting of:\n",
    "- An LSTM layer to process sequences of IMU sensor data.\n",
    "- A fully connected layer to classify the sequences into one of the six jump categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df78591d-8144-4826-a920-2bf22c7f5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JumpClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JumpClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=30, hidden_size=50, num_layers=4, batch_first=True)\n",
    "        self.fc = nn.Linear(50, 6)  # 6 categories\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # Get last time step\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = JumpClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33634eb6-3415-4142-b77b-07a56dafe6c7",
   "metadata": {},
   "source": [
    "## Prepare Data for Training\n",
    "This cell converts our loaded data into PyTorch tensors, splits it into training and validation sets, and prepares DataLoader objects for efficient data handling during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d66c35a-53aa-40c2-8ec9-c32eae49c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_data = torch.Tensor(data)\n",
    "tensor_labels = torch.LongTensor(labels)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(tensor_data, tensor_labels, test_size=0.3, random_state=42)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8529aae5-2124-4203-9df0-06684c7dc7e1",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Here, we define and execute the training loop for our model. The process involves:\n",
    "- Iterating over batches of training data.\n",
    "- Computing the loss and updating the model parameters using backpropagation.\n",
    "- Logging the loss to monitor the training progress.\n",
    "We use a CrossEntropyLoss for classification and the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13597c-7252-48bb-9d65-dc877bd3ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(200):  # Number of epochs\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd7ac1e-d38b-41a6-90e4-a813a4078fa5",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "After training, we evaluate the model's accuracy on the validation set. This step is crucial to understand the effectiveness of our model and identify any issues such as overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a25f9-3c29-4a29-afda-5711b4d495da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
